{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Indexing Techniques with LlamaIndex and Ollama: Part 2\n",
    "\n",
    "Welcome back to our deep dive into LlamaIndex and Ollama! In Part 1, we covered the essentials of setting up and using these powerful tools for efficient information retrieval. Now, it’s time to explore advanced indexing techniques that will elevate your document processing and querying capabilities to the next level.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Before we proceed, let’s quickly recap the key takeaways from Part 1:\n",
    "\n",
    "- Setting up LlamaIndex and Ollama\n",
    "- Creating a basic index\n",
    "- Performing simple queries\n",
    "\n",
    "In this part, we’ll dive into different index types, learn how to customize index settings, manage multiple documents, and explore advanced querying techniques. By the end, you’ll have a robust understanding of how to leverage LlamaIndex and Ollama for complex information retrieval tasks.\n",
    "\n",
    "If you haven’t set up your environment yet, make sure to refer back to Part 1 for detailed instructions on installing and configuring LlamaIndex and Ollama.\n",
    "\n",
    "## 2. Exploring Different Index Types\n",
    "\n",
    "LlamaIndex offers various index types, each tailored to different use cases. Let’s explore the four main types:\n",
    "\n",
    "### 2.1 List Index\n",
    "\n",
    "The List Index is the simplest form of indexing in LlamaIndex. It’s an ordered list of text chunks, ideal for straightforward use cases."
   ],
   "id": "847d3a10848a955"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from llama_index.core import ListIndex, SimpleDirectoryReader, VectorStoreIndex\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.ollama import  Ollama\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import chromadb\n",
    "from IPython.display import HTML\n",
    "# make markdown display text color green for all cells\n",
    "# Apply green color to all Markdown output\n",
    "def display_green_markdown(text):\n",
    "    green_style = \"\"\"\n",
    "    <style>\n",
    "    .green-output {\n",
    "        color: green;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    green_markdown = f'<div class=\"green-output\">{text}</div>'\n",
    "    display(HTML(green_style + green_markdown))\n",
    "\n",
    "\n",
    "# set the llm to ollama\n",
    "Settings.llm = Ollama(model='phi3', base_url='http://localhost:11434',temperature=0.1)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = ListIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is llama index used for?\")\n",
    "\n",
    "display_green_markdown(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Pros:**\n",
    "\n",
    "- Simple and quick to create\n",
    "- Best suited for small document sets\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Less efficient with large datasets\n",
    "- Limited semantic understanding\n",
    "\n",
    "### 2.2 Vector Store Index\n",
    "\n",
    "The Vector Store Index leverages embeddings to create a semantic representation of your documents, enabling more sophisticated searches."
   ],
   "id": "86efced540cf5d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Chroma client\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "# Define collection name\n",
    "collection_name = \"quickstart\"\n",
    "\n",
    "# Check if the collection already exists\n",
    "existing_collections = chroma_client.list_collections()\n",
    "\n",
    "if collection_name in [collection.name for collection in existing_collections]:\n",
    "    chroma_collection = chroma_client.get_collection(collection_name)\n",
    "    print(f\"Using existing collection '{collection_name}'.\")\n",
    "else:\n",
    "    chroma_collection = chroma_client.create_collection(collection_name)\n",
    "    print(f\"Created new collection '{collection_name}'.\")\n",
    "\n",
    "# Set up embedding model\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"prostatic\": 0},\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# Set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Create query engine and perform query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is llama index best suited for?\")\n",
    "display_green_markdown(response)\n"
   ],
   "id": "84cd29ceaf16ab39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "This index type excels in semantic search and scalability, making it ideal for large datasets.\n",
    "\n",
    "### 2.3 Tree Index\n",
    "\n",
    "The Tree Index organizes information hierarchically, which is beneficial for structured data."
   ],
   "id": "83a877bba22532fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "tree_index = TreeIndex.from_documents(documents)\n",
    "query_engine = tree_index.as_query_engine()\n",
    "response = query_engine.query(\"Explain the tree index structure.\")\n",
    "display_green_markdown(response)"
   ],
   "id": "8247024a601bea5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tree indices are particularly effective for data with natural hierarchies, such as organizational structures or taxonomies.\n",
    "\n",
    "### 2.4 Keyword Table Index\n",
    "\n",
    "The Keyword Table Index is optimized for efficient keyword-based retrieval."
   ],
   "id": "8c5719ff186a1193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import KeywordTableIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data/paul_graham').load_data()\n",
    "keyword_index = KeywordTableIndex.from_documents(documents)\n",
    "query_engine = keyword_index.as_query_engine()\n",
    "response = query_engine.query(\"What is the keyword table index in llama index?\")\n",
    "display_green_markdown(response)"
   ],
   "id": "9e31558e0fac2f0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This index type is ideal for scenarios that require quick lookups based on specific keywords.\n",
    "\n",
    "## 3. Customizing Index Settings\n",
    "\n",
    "### 3.1 Chunking Strategies\n",
    "\n",
    "Effective text chunking is crucial for index performance. LlamaIndex provides various chunking methods:"
   ],
   "id": "d63cc1798293cbcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(nodes[0])"
   ],
   "id": "b9ae5d133e22dee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Experiment with different chunking strategies to find the optimal balance between context preservation and query performance.\n",
    "\n",
    "### 3.2 Embedding Models\n",
    "\n",
    "LlamaIndex supports various embedding models. Here’s how you can use Ollama for embeddings:"
   ],
   "id": "e63bb4c795eda463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is an embedding model used for in LlamaIndex?\")\n",
    "display_green_markdown(response)"
   ],
   "id": "1076af39d6c34b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Experiment with different Ollama models and adjust parameters to optimize embedding quality for your specific use case.\n",
    "\n",
    "## 4. Handling Multiple Documents\n",
    "\n",
    "### 4.1 Creating a Multi-Document Index\n",
    "\n",
    "LlamaIndex simplifies the process of creating indices from multiple documents of various types:\n"
   ],
   "id": "967aba14da015b1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "txt_docs = SimpleDirectoryReader('data/paul_graham').load_data()\n",
    "web_docs = SimpleDirectoryReader('web_pages').load_data()\n",
    "data = txt_docs  + web_docs\n",
    "all_docs = txt_docs  + web_docs\n",
    "index = VectorStoreIndex.from_documents(all_docs)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"How do you create a multi-document index in LlamaIndex?\")\n",
    "display_green_markdown(response)"
   ],
   "id": "e85767f897b1fef7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Cross-Document Querying\n",
    "\n",
    "To effectively query across multiple documents, you can implement relevance scoring and manage context boundaries:"
   ],
   "id": "b3c8e098a345950a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\"compact\")\n",
    "query = QueryBundle(\"How do you query across multiple documents?\")\n",
    "response = query_engine.query(query)\n",
    "display_green_markdown(response)"
   ],
   "id": "164bdc39c43a24c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "In this second part of our LlamaIndex and Ollama series, we explored advanced indexing techniques, including:\n",
    "\n",
    "- Different index types and their use cases\n",
    "- Customizing index settings for optimal performance\n",
    "- Handling multiple documents and cross-document querying\n",
    "\n"
   ],
   "id": "1b6857989d127eb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
