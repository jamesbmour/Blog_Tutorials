{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Advanced Indexing Techniques with LlamaIndex and Ollama: Part 2\n",
    "\n",
    "Welcome back to our deep dive into LlamaIndex and Ollama! In Part 1, we covered the essentials of setting up and using these powerful tools for efficient information retrieval. Now, it’s time to explore advanced indexing techniques that will elevate your document processing and querying capabilities to the next level.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Before we proceed, let’s quickly recap the key takeaways from Part 1:\n",
    "\n",
    "- Setting up LlamaIndex and Ollama\n",
    "- Creating a basic index\n",
    "- Performing simple queries\n",
    "\n",
    "In this part, we’ll dive into different index types, learn how to customize index settings, manage multiple documents, and explore advanced querying techniques. By the end, you’ll have a robust understanding of how to leverage LlamaIndex and Ollama for complex information retrieval tasks.\n",
    "\n",
    "If you haven’t set up your environment yet, make sure to refer back to Part 1 for detailed instructions on installing and configuring LlamaIndex and Ollama.\n",
    "\n",
    "## 2. Exploring Different Index Types\n",
    "\n",
    "LlamaIndex offers various index types, each tailored to different use cases. Let’s explore the four main types:\n",
    "\n",
    "### 2.1 List Index\n",
    "\n",
    "The List Index is the simplest form of indexing in LlamaIndex. It’s an ordered list of text chunks, ideal for straightforward use cases."
   ],
   "id": "847d3a10848a955"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T12:08:57.488757Z",
     "start_time": "2024-08-14T12:08:56.393721Z"
    }
   },
   "source": [
    "from llama_index.core import ListIndex, SimpleDirectoryReader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = ListIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the capital of France?\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex provides a simple interface to query your data using natural language.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Pros:**\n",
    "\n",
    "- Simple and quick to create\n",
    "- Best suited for small document sets\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Less efficient with large datasets\n",
    "- Limited semantic understanding\n",
    "\n",
    "### 2.2 Vector Store Index\n",
    "\n",
    "The Vector Store Index leverages embeddings to create a semantic representation of your documents, enabling more sophisticated searches."
   ],
   "id": "86efced540cf5d5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:15:09.673631Z",
     "start_time": "2024-08-14T12:15:00.133765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"{response}\"))"
   ],
   "id": "84cd29ceaf16ab39",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "The author spent most of 2014 painting after deciding to do something completely different following his work with Y Combinator."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 Tree Index",
   "id": "83a877bba22532fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:15:32.017778Z",
     "start_time": "2024-08-14T12:15:29.761511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import TreeIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "tree_index = TreeIndex.from_documents(documents)\n",
    "query_engine = tree_index.as_query_engine()\n",
    "response = query_engine.query(\"Explain the structure of the human respiratory system.\")\n",
    "print(response)"
   ],
   "id": "8247024a601bea5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a powerful data framework designed to help developers build AI applications with large language models (LLMs). It provides a set of tools and techniques to connect custom data sources to LLMs, enabling more accurate and context-aware responses. Key features include data ingestion, data indexing, a query interface, LLM integration, customization options, and scalability. Use cases for LlamaIndex include question-answering systems, chatbots with domain-specific knowledge, semantic search applications, and document analysis and summarization.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.4 Keyword Table Index",
   "id": "8344394e9310ef89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:16:36.499002Z",
     "start_time": "2024-08-14T12:16:03.686140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import KeywordTableIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('data/paul_graham').load_data()\n",
    "keyword_index = KeywordTableIndex.from_documents(documents)\n",
    "query_engine = keyword_index.as_query_engine()\n",
    "response = query_engine.query(\"What are the symptoms of influenza?\")\n",
    "print(response)"
   ],
   "id": "b13df20874fb8a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Customizing Index Settings\n",
    "\n",
    "### 3.1 Chunking Strategies"
   ],
   "id": "d63cc1798293cbcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:16:41.258645Z",
     "start_time": "2024-08-14T12:16:41.254098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
    "# Sentence-based chunkingparser = SimpleNodeParser.from_defaults(chunk_size=None, chunk_overlap=0)\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(nodes[0])"
   ],
   "id": "b9ae5d133e22dee4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 42190b55-9682-457e-a22c-d8c9bd28869c\n",
      "Text: LlamaIndex: An Introduction  LlamaIndex is a powerful data\n",
      "framework designed to help developers build AI applications with large\n",
      "language models (LLMs). It provides a set of tools and techniques to\n",
      "connect custom data sources to LLMs, enabling more accurate and\n",
      "context-aware responses.  Key Features of LlamaIndex:  1. Data\n",
      "Ingestion: LlamaIndex...\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 Embedding Models",
   "id": "e63bb4c795eda463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T11:03:54.301668Z",
     "start_time": "2024-08-14T11:03:41.887844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"snowflake-arctic-embed\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are the main themes in Shakespeare's plays?\")\n",
    "print(response)"
   ],
   "id": "1076af39d6c34b50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a powerful data framework designed to help developers build AI applications with large language models (LLMs). It provides a set of tools and techniques to connect custom data sources to LLMs, enabling more accurate and context-aware responses. Key features of LlamaIndex include data ingestion, data indexing, query interface, LLM integration, customization, and scalability. Use cases for LlamaIndex include question-answering systems, chatbots with domain-specific knowledge, semantic search applications, and document analysis and summarization.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Handling Multiple Documents\n",
    "\n",
    "### 4.1 Creating a Multi-Document Index"
   ],
   "id": "967aba14da015b1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:19:08.125657Z",
     "start_time": "2024-08-14T12:19:06.040140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load documents from different sourcespdf_docs = SimpleDirectoryReader('pdfs').load_data()\n",
    "txt_docs = SimpleDirectoryReader('data/paul_graham').load_data()\n",
    "# csv_docs = SimpleDirectoryReader('csvs').load_data()\n",
    "web_docs = SimpleDirectoryReader('web_pages').load_data()\n",
    "data = txt_docs  + web_docs\n",
    "all_docs = txt_docs  + web_docs\n",
    "index = VectorStoreIndex.from_documents(all_docs)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what os llama index\")\n",
    "print(response)"
   ],
   "id": "e85767f897b1fef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a tool that offers various index types for different use cases. It is a tool used for efficient information retrieval and document processing tasks.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Cross-Document Querying",
   "id": "b3c8e098a345950a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:19:20.038797Z",
     "start_time": "2024-08-14T12:19:17.721629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, response_mode=\"compact\")\n",
    "query = QueryBundle(\"What are the common themes across all documents?\")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ],
   "id": "164bdc39c43a24c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common themes across all documents include personal experiences and reflections, career development, technological advancements, educational pursuits, and the intersection of art and technology. The documents touch on topics such as the author's journey in writing and programming, the evolution of computers from mainframes to microcomputers, the importance of working on unprestigious projects, the challenges and rewards of pursuing art and painting, the significance of studying philosophy and transitioning to AI, and the impact of the World Wide Web on society.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Updating and Managing Indices\n",
    "## 5.1 Adding New Documents to Existing Indices"
   ],
   "id": "e8b21d6bc2cf9cf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:19:43.004487Z",
     "start_time": "2024-08-14T12:19:42.988978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_doc = SimpleDirectoryReader('data/doc-1.txt').load_data()[0]\n",
    "index.insert(new_doc)"
   ],
   "id": "95ba5b8985fd9be",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Directory data/doc-1.txt does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m new_doc \u001B[38;5;241m=\u001B[39m SimpleDirectoryReader(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/doc-1.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mload_data()[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      2\u001B[0m index\u001B[38;5;241m.\u001B[39minsert(new_doc)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/readers/file/base.py:260\u001B[0m, in \u001B[0;36mSimpleDirectoryReader.__init__\u001B[0;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_dir:\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39misdir(input_dir):\n\u001B[0;32m--> 260\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDirectory \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_dir \u001B[38;5;241m=\u001B[39m _Path(input_dir)\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexclude \u001B[38;5;241m=\u001B[39m exclude\n",
      "\u001B[0;31mValueError\u001B[0m: Directory data/doc-1.txt does not exist."
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.3 Index Persistence and Serialization",
   "id": "265573a140d9f9cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:20:34.819479Z",
     "start_time": "2024-08-14T12:20:34.421816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "index.storage_context.persist(\"save_dir\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"save_dir\")\n",
    "loaded_index = load_index_from_storage(storage_context)"
   ],
   "id": "cd25c28f338c334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x142df0950>\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Advanced Querying Techniques\n",
    "\n",
    "### 6.1 Query Preprocessing"
   ],
   "id": "871fe87e465490ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:21:02.148182Z",
     "start_time": "2024-08-14T12:20:59.668316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "def preprocess_query(query):\n",
    "    # Normalize text    query = query.lower()\n",
    "    # Remove special characters    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    # Expand common abbreviations    query = query.replace(\"ai\", \"artificial intelligence\")\n",
    "    return query\n",
    "preprocessed_query = preprocess_query(\"What's the latest in AI?\")\n",
    "response = query_engine.query(preprocessed_query)\n",
    "print(response)"
   ],
   "id": "88f50853823fce22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest in AI involves advancements in various fields such as natural language processing, computer vision, and machine learning. Researchers are working on developing more sophisticated AI models that can understand and generate human-like text, recognize objects and patterns in images, and improve decision-making processes. Additionally, there is a focus on creating AI systems that can learn from limited data and adapt to new environments, pushing the boundaries of what AI can achieve in terms of intelligence and autonomy.\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.2 Hybrid Search Strategies",
   "id": "7553d0ed40d616b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:21:13.743803Z",
     "start_time": "2024-08-14T12:21:13.709259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "keyword_retriever = index.as_retriever(mode=\"keyword\")\n",
    "vector_retriever = index.as_retriever(mode=\"embedding\")\n",
    "def hybrid_retriever(query_bundle: QueryBundle):\n",
    "    keyword_nodes = keyword_retriever.retrieve(query_bundle)\n",
    "    vector_nodes = vector_retriever.retrieve(query_bundle)\n",
    "    return list(set(keyword_nodes + vector_nodes))\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(hybrid_retriever)\n",
    "response = query_engine.query(\"What are the environmental impacts of renewable energy?\")\n",
    "print(response)\n"
   ],
   "id": "e83d1b64a895dd18",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'retrieve'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(keyword_nodes \u001B[38;5;241m+\u001B[39m vector_nodes))\n\u001B[1;32m      7\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m RetrieverQueryEngine\u001B[38;5;241m.\u001B[39mfrom_args(hybrid_retriever)\n\u001B[0;32m----> 8\u001B[0m response \u001B[38;5;241m=\u001B[39m query_engine\u001B[38;5;241m.\u001B[39mquery(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat are the environmental impacts of renewable energy?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    253\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_,\n\u001B[1;32m    254\u001B[0m     bound_args\u001B[38;5;241m=\u001B[39mbound_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    257\u001B[0m     tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[1;32m    258\u001B[0m )\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 260\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py:52\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     51\u001B[0m         str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[0;32m---> 52\u001B[0m     query_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query(str_or_query_bundle)\n\u001B[1;32m     53\u001B[0m dispatcher\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m     54\u001B[0m     QueryEndEvent(query\u001B[38;5;241m=\u001B[39mstr_or_query_bundle, response\u001B[38;5;241m=\u001B[39mquery_result)\n\u001B[1;32m     55\u001B[0m )\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m query_result\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[0;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[1;32m    253\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_,\n\u001B[1;32m    254\u001B[0m     bound_args\u001B[38;5;241m=\u001B[39mbound_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    257\u001B[0m     tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[1;32m    258\u001B[0m )\n\u001B[1;32m    259\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 260\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:189\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    187\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str}\n\u001B[1;32m    188\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[0;32m--> 189\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve(query_bundle)\n\u001B[1;32m    190\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_synthesizer\u001B[38;5;241m.\u001B[39msynthesize(\n\u001B[1;32m    191\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery_bundle,\n\u001B[1;32m    192\u001B[0m         nodes\u001B[38;5;241m=\u001B[39mnodes,\n\u001B[1;32m    193\u001B[0m     )\n\u001B[1;32m    194\u001B[0m     query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response})\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:144\u001B[0m, in \u001B[0;36mRetrieverQueryEngine.retrieve\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mretrieve\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[NodeWithScore]:\n\u001B[0;32m--> 144\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retriever\u001B[38;5;241m.\u001B[39mretrieve(query_bundle)\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_node_postprocessors(nodes, query_bundle\u001B[38;5;241m=\u001B[39mquery_bundle)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'retrieve'"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "add3ca391cb86b5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
