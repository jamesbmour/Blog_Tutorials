{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain Part 4 - Leveraging Memory and Storage in LangChain: A Comprehensive Guide\n",
    "\n",
    "In the ever-evolving world of conversational AI and language models, maintaining context and efficiently managing information flow are critical components of building intelligent applications. LangChain, a powerful framework designed for working with large language models (LLMs), offers robust tools for memory management and data persistence, enabling the creation of context-aware systems.\n",
    "\n",
    "In this guide, we'll delve into the nuances of leveraging memory and storage in LangChain to build smarter, more responsive applications.\n",
    "\n",
    "## 1. Working with Memory in LangChain\n",
    "\n",
    "Memory management in LangChain allows applications to retain context, making interactions more coherent and contextually relevant. Let’s explore the different memory types and their use cases.\n",
    "\n",
    "### 1.1. Types of Memory\n",
    "\n",
    "LangChain provides various memory types to address different scenarios. Here, we’ll focus on two key types:\n",
    "\n",
    "**ConversationBufferMemory**\n",
    "\n",
    "This memory type is ideal for short-term context retention, capturing and recalling recent interactions in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:48.635830Z",
     "start_time": "2024-08-21T12:46:48.633048Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi, I'm Alice\"}, {\"output\": \"Hello Alice, how can I help you today?\"})\n",
    "memory.save_context({\"input\": \"What's the weather like?\"}, {\"output\": \"I'm sorry, I don't have real-time weather information. Is there anything else I can help you with?\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: Hi, I'm Alice\\nAI: Hello Alice, how can I help you today?\\nHuman: What's the weather like?\\nAI: I'm sorry, I don't have real-time weather information. Is there anything else I can help you with?\"}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**ConversationSummaryMemory**\n",
    "\n",
    "For longer conversations, **ConversationSummaryMemory** is a great choice. It summarizes key points, maintaining context without overwhelming detail."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:51.541398Z",
     "start_time": "2024-08-21T12:46:48.655070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import Ollama \n",
    "# set llm model`\n",
    "\n",
    "\n",
    "llm = Ollama(model='phi3',temperature=0)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory.save_context({\"input\": \"Hi, I'm Alice\"}, {\"output\": \"Hello Alice, how can I help you today?\"})\n",
    "memory.save_context({\"input\": \"I'm looking for a good Italian restaurant\"}, {\"output\": \"Great! I'd be happy to help you find a good Italian restaurant. Do you have any specific preferences or requirements, such as location, price range, or specific dishes you're interested in?\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"The human introduces themselves as Alice and asks the AI for assistance with something they need at that moment. The next day, Alice expresses her desire to find a good Italian restaurant, prompting the AI to offer help by asking about specific preferences or requirements such as location, price range, or particular dishes she'd like to try.\"}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2. Choosing the Right Memory Type for Your Use Case\n",
    "\n",
    "Selecting the appropriate memory type depends on several factors:\n",
    "\n",
    "- **Duration and Complexity**: Short sessions benefit from detailed context retention with ConversationBufferMemory, while long-term interactions may require summarization via ConversationSummaryMemory.\n",
    "- **Detail vs. Overview**: Determine whether detailed interaction history or high-level summaries are more valuable for your application.\n",
    "- **Performance**: Consider the trade-offs between the memory size and retrieval speed.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- **ConversationBufferMemory**: Ideal for quick customer support or FAQ-style interactions.\n",
    "- **ConversationSummaryMemory**: Best suited for long-term engagements like project management or ongoing customer interactions.\n",
    "\n",
    "### 1.3. Integrating Memory into Chains and Agents\n",
    "\n",
    "Memory can be seamlessly integrated into LangChain chains and agents to enhance conversational capabilities."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:53.566686Z",
     "start_time": "2024-08-21T12:46:51.620730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationChain  \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi, I'm Alice\")\n",
    "conversation.predict(input=\"What's my name?\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm Alice\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm Alice\n",
      "AI: Hello Alice! It's wonderful to meet you. How can I assist you today? Do share any specific interests or topics that pique your curiosity so we can dive into an engaging discussion together. Remember, there are no silly questions here—I'm always ready with information and insights on a wide range of subjects!\n",
      "Human: What's my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Alice, as you mentioned at the beginning of our conversation. It’s lovely to have your company today, Alice. How may I help further in making this interaction even more delightful for you? Whether it's about hobbies, science facts, or just sharing a laugh over an interesting tidbit, feel free to ask away!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:55.420694Z",
     "start_time": "2024-08-21T12:46:53.580166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi, I'm Alice\")\n",
    "conversation.predict(input=\"What's my name?\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I'm Alice\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I'm Alice\n",
      "AI: Hello Alice! It's wonderful to meet you. How can I assist you today? Do share any specific interests or topics that pique your curiosity so we can dive into an engaging discussion together. Remember, there are no silly questions here—I'm always ready with information and insights on a wide range of subjects!\n",
      "Human: What's my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Alice, as you mentioned at the beginning of our conversation. It’s lovely to have your company today, Alice. How may I help further in making this interaction even more delightful for you? Whether it's about hobbies, science facts, or just sharing a laugh over an interesting tidbit, feel free to ask away!\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This example illustrates how **ConversationBufferMemory** can be used to remember previous interactions, enabling more natural conversations.\n",
    "\n",
    "## 2. Persisting and Retrieving Data\n",
    "\n",
    "Persistent storage ensures that conversation history and context are maintained across sessions, enabling continuity in interactions.\n",
    "\n",
    "### 2.1. Storing Conversation History and State\n",
    "\n",
    "For basic persistence, you can use file-based storage with JSON:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:55.442995Z",
     "start_time": "2024-08-21T12:46:55.438807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class PersistentMemory:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.load_memory()\n",
    "\n",
    "    def load_memory(self):\n",
    "        try:\n",
    "            with open(self.file_path, 'r') as f:\n",
    "                self.chat_memory = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.chat_memory = {'messages': []}\n",
    "\n",
    "    def save_memory(self):\n",
    "        with open(self.file_path, 'w') as f:\n",
    "            json.dump({'messages': self.chat_memory['messages']}, f)\n",
    "\n",
    "# Usage\n",
    "memory = PersistentMemory(file_path='conversation_history.json')\n",
    "print(memory.chat_memory)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'type': 'human', 'data': {'content': 'Hello, how are you?', 'additional_kwargs': {}, 'example': False}}, {'type': 'ai', 'data': {'content': \"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you. How can I help you today?\", 'additional_kwargs': {}, 'example': False}}]}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method allows you to persist conversation history in a simple, human-readable format.\n",
    "\n",
    "### 2.2. Integrating with Databases and Storage Systems\n",
    "\n",
    "For more scalable and efficient storage, integrating with databases like SQLite is recommended:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:55.460214Z",
     "start_time": "2024-08-21T12:46:55.456560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# store memory in SQLite database\n",
    "import sqlite3\n",
    "\n",
    "class SQLiteMemory:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS conversations\n",
    "            (id INTEGER PRIMARY KEY, input TEXT, output TEXT)\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('INSERT INTO conversations (input, output) VALUES (?, ?)',\n",
    "                       (inputs['input'], outputs['output']))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('SELECT input, output FROM conversations ORDER BY id DESC LIMIT 10')\n",
    "        rows = cursor.fetchall()\n",
    "        history = \"\\\\n\".join([f\"Human: {row[0]}\\\\nAI: {row[1]}\" for row in reversed(rows)])\n",
    "        return {\"history\": history}\n",
    "    \n",
    "# Usage\n",
    "memory = SQLiteMemory('conversation_history.db')\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': ''}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This implementation provides robust storage with the ability to handle large-scale applications and complex queries.\n",
    "\n",
    "## 3. Advanced Memory Techniques\n",
    "\n",
    "For more specialized scenarios, consider implementing custom memory classes or combining multiple memory types.\n",
    "\n",
    "### 3.1. Implementing Custom Memory Classes\n",
    "\n",
    "A custom memory class can be tailored to specific needs, such as maintaining a fixed-size buffer:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:46:55.490961Z",
     "start_time": "2024-08-21T12:46:55.489488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Implementing Custom Memory Classes\n",
    "# # A custom memory class can be tailored to specific needs, such as maintaining a fixed-size buffer:\n",
    "# \n",
    "# from langchain.memory import \n",
    "# from collections import deque\n",
    "# \n",
    "# class FixedSizeMemory(BaseMemory):\n",
    "#     def __init__(self, k=5):\n",
    "#         self.k = k\n",
    "#         self.memory = deque(maxlen=k)\n",
    "# \n",
    "#     def save_context(self, inputs, outputs):\n",
    "#         self.memory.append((inputs['input'], outputs['output']))\n",
    "# \n",
    "#     def load_memory_variables(self, inputs):\n",
    "#         return {\"history\": \"\\\\n\".join([f\"Human: {i}\\\\nAI: {o}\" for i, o in self.memory])}\n",
    "# \n",
    "#     def clear(self):\n",
    "#         self.memory.clear()\n",
    "#         \n",
    "# # Usage\n",
    "# memory = FixedSizeMemory(k=3)\n",
    "# \n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:49:21.737916Z",
     "start_time": "2024-08-21T12:49:21.728459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain.memory import BaseMemory\n",
    "from collections import deque\n",
    "from langchain.schema import BaseMemory\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class FixedSizeMemory(BaseMemory, BaseModel):\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.memory = deque(maxlen=k)\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        self.memory.append((inputs['input'], outputs['output']))\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        return {\"history\": \"\\\\n\".join([f\"Human: {i}\\\\nAI: {o}\" for i, o in self.memory])}\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "# Usage\n",
    "memory = FixedSizeMemory(k=3)\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseMemory\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mFixedSizeMemory\u001B[39;00m(BaseMemory, BaseModel):\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk \u001B[38;5;241m=\u001B[39m k\n",
      "\u001B[0;31mTypeError\u001B[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2. Combining Multiple Memory Types\n",
    "\n",
    "For enhanced functionality, you can combine different memory types:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:50:53.707570Z",
     "start_time": "2024-08-21T12:50:53.689069Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/memory/combined.py:38: UserWarning: When using CombinedMemory, input keys should be so the input is known.  Was not set on memory_key='buffer_memory'\n",
      "  warnings.warn(\n",
      "/Users/james/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/memory/combined.py:38: UserWarning: When using CombinedMemory, input keys should be so the input is known.  Was not set on llm=Ollama(model='phi3', temperature=0.0) memory_key='summary_memory'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['buffer_memory', 'summary_memory'] as inputs from memory, and input as the normal input key. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Use in a chain\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConversationChain\n\u001B[0;32m---> 13\u001B[0m conversation \u001B[38;5;241m=\u001B[39m ConversationChain(\n\u001B[1;32m     14\u001B[0m     llm\u001B[38;5;241m=\u001B[39mllm,\n\u001B[1;32m     15\u001B[0m     memory\u001B[38;5;241m=\u001B[39mcombined_memory,\n\u001B[1;32m     16\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     17\u001B[0m )\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[1;32m     20\u001B[0m conversation\u001B[38;5;241m.\u001B[39mpredict(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHi, I\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm Alice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:205\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    204\u001B[0m     emit_warning()\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:205\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    204\u001B[0m     emit_warning()\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/langchain_core/load/serializable.py:113\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    112\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[1;32m    339\u001B[0m values, fields_set, validation_error \u001B[38;5;241m=\u001B[39m validate_model(__pydantic_self__\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, data)\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_error:\n\u001B[0;32m--> 341\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m validation_error\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     object_setattr(__pydantic_self__, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__dict__\u001B[39m\u001B[38;5;124m'\u001B[39m, values)\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['buffer_memory', 'summary_memory'] as inputs from memory, and input as the normal input key. (type=value_error)"
     ]
    }
   ],
   "execution_count": 45,
   "source": [
    "from langchain.memory import CombinedMemory, ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "# Create individual memory instances\n",
    "buffer_memory = ConversationBufferMemory(memory_key=\"buffer_memory\")\n",
    "summary_memory = ConversationSummaryMemory(llm=llm, memory_key=\"summary_memory\")\n",
    "\n",
    "# Combine memories\n",
    "combined_memory = CombinedMemory(memories=[buffer_memory, summary_memory])\n",
    "\n",
    "# Use in a chain\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=combined_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Usage\n",
    "conversation.predict(input=\"Hi, I'm Alice\")\n",
    "conversation.predict(input=\"Tell me about our conversation so far\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This approach allows your application to maintain both detailed short-term memory and summarized long-term memory, making it more adaptable to different interaction styles.\n",
    "\n",
    "### 3.3. Optimizing Memory Usage and Performance\n",
    "\n",
    "To ensure your application remains responsive, consider these optimization strategies:\n",
    "\n",
    "- **Efficient Data Structures**: Use structures like `deque` for managing fixed-size buffers.\n",
    "- **Caching Strategies**: Reduce database queries by implementing caching for frequently accessed data.\n",
    "- **Data Pruning**: Regularly prune or summarize old data to maintain a manageable memory size.\n",
    "\n",
    "Here’s an example of a memory class with basic caching:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:50:47.454845Z",
     "start_time": "2024-08-21T12:50:47.451911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "class CachedSQLiteMemory(SQLiteMemory):\n",
    "    def __init__(self, db_path, cache_ttl=60):\n",
    "        super().__init__(db_path)\n",
    "        self.cache = None\n",
    "        self.cache_time = 0\n",
    "        self.cache_ttl = cache_ttl\n",
    "        \n",
    "    def load_memory_variables(self, inputs):\n",
    "        current_time = time.time()\n",
    "        if self.cache is None or (current_time - self.cache_time) > self.cache_ttl:\n",
    "            var = self.cache\n",
    "            self.cache = super().load_memory_variables(inputs)\n",
    "            self.cache_time = current_time\n",
    "            return self.cache\n",
    "\n",
    "memory = CachedSQLiteMemory('conversation_history.db', cache_ttl=30)\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This implementation caches the results of database queries for a specified time, reducing the load on the database and improving performance for applications that frequently access memory data.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Effective memory management is a cornerstone of building intelligent, context-aware conversational AI applications. LangChain provides a flexible and powerful framework for managing memory, allowing developers to tailor memory types to specific use cases, implement persistent storage solutions, and optimize performance for large-scale applications.\n",
    "\n",
    "By choosing the right memory type, integrating persistent storage, and leveraging advanced techniques such as custom memory classes and caching strategies, you can build sophisticated AI systems that maintain context, improve user experience, and operate efficiently even as the scale and complexity of interactions grow.\n",
    "\n",
    "With these tools and techniques at your disposal, you are well-equipped to harness the full potential of LangChain in creating responsive, intelligent, and contextually aware AI applications. Whether you’re developing customer support bots, virtual assistants, or complex conversational systems, mastering memory and storage in LangChain will be a key factor in your success."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
