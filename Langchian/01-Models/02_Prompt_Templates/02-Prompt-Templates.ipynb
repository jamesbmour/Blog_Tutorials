{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:14:10.954362Z",
     "start_time": "2024-11-09T10:14:01.837631Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d54e742",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a language model, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have. How can I assist you today?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    "\n",
    ")\n",
    "\n",
    "result = llm.invoke(\"Hello, how are you today?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac07c035405c631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:20:54.477923Z",
     "start_time": "2024-11-09T10:20:54.473539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a fact about Moon.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Create template\n",
    "pt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a fact about {topic}.\"\n",
    ")\n",
    "\n",
    "# Use template\n",
    "formatted_prompt = pt.format(topic=\"Moon\")\n",
    "print(formatted_prompt)  # Outputs: Tell me a fact about Moon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875f848b995ffa49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:26:02.596288Z",
     "start_time": "2024-11-09T10:26:02.592927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact about Moon for a student 6th Grade level.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a template for creating prompts that require multiple input variables: 'topic' and 'level'\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],  # Specify the required input variables\n",
    "    template=\"Tell me a fact about {topic} for a student {level} level.\"  # Template string with placeholders for inputs\n",
    ")\n",
    "\n",
    "# Format the prompt by replacing the placeholders in the template with actual values: 'Moon' for topic and '6th Grade' for level\n",
    "multiple_input_prompt.format(topic='Moon', level='6th Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a290e4df1e0303ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:30:13.577037Z",
     "start_time": "2024-11-09T10:30:12.181130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a cool fact about the Moon that a 6th-grade student might enjoy:\n",
      "\n",
      "Did you know that the Moon is actually moving away from the Earth at a rate of about 1.5 inches (3.8 centimeters) every year? This is because the Moon's orbit is slowly increasing in size due to the tidal interactions between the Earth and the Moon. This process is called \"tidal acceleration.\" It's been happening for billions of years, and the Moon is now about \n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(multiple_input_prompt.format(topic='Moon', level='6th Grade'))\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc58b6709bce8c",
   "metadata": {},
   "source": [
    "# ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cd112837f36773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:28:51.778895Z",
     "start_time": "2024-11-09T10:28:51.776086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "# prompt_template.format(topic=\"chickens\")\n",
    "p = prompt_template.invoke({\"topic\": \"cats\"}).to_messages()\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a03f01d8a5e5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:28:56.744131Z",
     "start_time": "2024-11-09T10:28:55.237564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist.', additional_kwargs={'refusal': ''}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 46, 'total_tokens': 66, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/llama-3.2-3b-instruct', 'system_fingerprint': 'fastcoe', 'finish_reason': 'stop', 'logprobs': None}, id='run-8811a11f-4453-45dd-95c3-ac06bf8b29f1-0', usage_metadata={'input_tokens': 46, 'output_tokens': 20, 'total_tokens': 66, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add5052d8bafd44",
   "metadata": {},
   "source": [
    "# MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a5612d351d7f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:32:26.690831Z",
     "start_time": "2024-11-09T10:32:26.687585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Define a chat prompt template with a system message and a placeholder for user messages\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),  # System message indicating the assistant's role\n",
    "    MessagesPlaceholder(\"msgs\")  # Placeholder where user messages will be inserted\n",
    "])\n",
    "\n",
    "# Invoke the prompt template with a list containing a single human message\n",
    "p = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})  # User sends \"hi!\" to the assistant\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34df18200c4184f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:32:43.820210Z",
     "start_time": "2024-11-09T10:32:43.291277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = llm.invoke(p)\n",
    "\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6862831aa7299",
   "metadata": {},
   "source": [
    "# Few-Shot PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f2789169cf3c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:39:50.192886Z",
     "start_time": "2024-11-09T10:39:50.189254Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Define a list of examples with input and corresponding output\n",
    "examples = [\n",
    "    {\"input\": \"2 ðŸ¦œ 2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2 ðŸ¦œ 3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\", \"{input}?\"),\n",
    "        (\"ai\", \"{output}.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2340c97a5325b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:41:15.744802Z",
     "start_time": "2024-11-09T10:41:15.742471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples=[{'input': '2 ðŸ¦œ 2', 'output': '4'}, {'input': '2 ðŸ¦œ 3', 'output': '5'}] input_variables=[] input_types={} partial_variables={} example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}.'), additional_kwargs={})])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52282cf674ba5ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:41:41.095172Z",
     "start_time": "2024-11-09T10:41:41.092571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='2 ðŸ¦œ 2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='2 ðŸ¦œ 3?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='5.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = few_shot_prompt.invoke({}).to_messages()\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4c451315a20117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:41:58.325770Z",
     "start_time": "2024-11-09T10:41:54.098113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're using emojis to represent numbers. I'll follow along.\\n\\n2 birds (ðŸ¦œ) 2 = 4\\n2 birds (ðŸ¦œ) 3 = 5\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = llm.invoke(p)\n",
    "r.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17494c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-tutorials-M8bgSKgB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
